{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d863fd1",
   "metadata": {},
   "source": [
    "## Cluster Finetuning notebook for Divan10 Case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e7678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from loading_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "HUGGINGFACE_TOKEN = \"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\"\n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e0fbf",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "480352dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/scratch/students/nour/.conda/envs/PDS/lib/python3.11/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce GTX TITAN X which is of cuda capability 5.2.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/scratch/students/nour/.conda/envs/PDS/lib/python3.11/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/scratch/students/nour/.conda/envs/PDS/lib/python3.11/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU1 NVIDIA GeForce GTX TITAN X which is of cuda capability 5.2.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/scratch/students/nour/.conda/envs/PDS/lib/python3.11/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "NVIDIA GeForce GTX TITAN X with CUDA capability sm_52 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce GTX TITAN X GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d069b4d99b2e496892c28ecd9b262222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch \n",
    "\n",
    "\n",
    "model_id = \"ArmGPT/ArmenianGPT-0.1-12B\"\n",
    "\n",
    "# BitsAndBytesConfig for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                   # enable 4-bit loading\n",
    "    bnb_4bit_use_double_quant=True,      # nested quantization for stability\n",
    "    bnb_4bit_quant_type=\"nf4\",           # best quantization type for LLMs\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # compute in bf16 (safer than fp16 on older GPUs)\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/rcp-scratch/iccluster040_scratch/students/nour/hf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb43de",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15d7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_ner_json_files(train_path, eval_path, test_path):\n",
    "    \"\"\"\n",
    "    Loads train, eval, and test JSON files into a DatasetDict.\n",
    "\n",
    "    Args:\n",
    "        train_path (str or Path): path to train JSON file\n",
    "        eval_path  (str or Path): path to eval JSON file\n",
    "        test_path  (str or Path): path to test JSON file\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict with keys: 'train', 'eval', 'test'\n",
    "    \"\"\"\n",
    "    def load_json_to_df(path):\n",
    "        # If JSON lines format\n",
    "        try:\n",
    "            return pd.read_json(path, lines=True)\n",
    "        except ValueError:\n",
    "            # Fallback: normal JSON array\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            return pd.DataFrame(data)\n",
    "    \n",
    "    train_df = load_json_to_df(train_path)\n",
    "    eval_df  = load_json_to_df(eval_path)\n",
    "    test_df  = load_json_to_df(test_path)\n",
    "\n",
    "    # Convert pandas DataFrames to Hugging Face Datasets\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    eval_ds  = Dataset.from_pandas(eval_df)\n",
    "    test_ds  = Dataset.from_pandas(test_df)\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_ds,\n",
    "        \"validation\": eval_ds,\n",
    "        \"test\": test_ds\n",
    "    })\n",
    "\n",
    "\n",
    "dataset=load_ner_json_files(\"train_ner.jsonl\",\"eval_ner.jsonl\",\"test_ner.jsonl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb18412",
   "metadata": {},
   "source": [
    "###  Map empty outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b1940d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f422e0b361e4cf5a7e1f46a84602536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604b6411576442e9ac8fb7ad21059579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f160ccdcce034e4cb733a02b96a91074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_output(example):\n",
    "    if not example[\"output\"].strip():\n",
    "        example[\"output\"] = \"O\"  # or \"No entities found.\"\n",
    "    return example\n",
    "\n",
    "dataset = {\n",
    "    split: ds.map(normalize_output)\n",
    "    for split, ds in dataset.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f061eb2e",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a187432f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca74ab8b9b94acda3b70144c9e6cb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9537a0a7e34e0f9c704d6eb7847a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa0872f61e344f4892e40a561cd3fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 256 \n",
    "max_target_length = 128\n",
    "def preprocess(example):\n",
    "\n",
    "    output_text = example[\"output\"].strip()\n",
    "    if not output_text:\n",
    "        output_text = \"[]\"  # or \"No entities found.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are an Armenian AI assistant for NER tasks.\"}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": example[\"input\"]}]},\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": output_text}]},\n",
    "    ]\n",
    "\n",
    "    processed = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,\n",
    "        max_length=max_input_length + max_target_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    input_ids = processed[\"input_ids\"][0]\n",
    "    attention_mask = processed[\"attention_mask\"][0]\n",
    "    \n",
    "    labels = input_ids.copy()\n",
    "    labels = [tok if mask == 1 else -100 for tok, mask in zip(input_ids, attention_mask)]\n",
    "    if len(labels) > 1:\n",
    "        labels[-1] = -100\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "tokenized_dataset = { split: ds.map(preprocess, batched=False, remove_columns=ds.column_names) for split, ds in dataset.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6908ce",
   "metadata": {},
   "source": [
    "## Load Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31fb6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./gemma3_ner\", per_device_train_batch_size=1,\n",
    "                                   per_device_eval_batch_size=1, gradient_accumulation_steps=4, num_train_epochs=2,\n",
    "                                     learning_rate=5e-5, save_strategy=\"steps\", save_steps=200, eval_strategy=\"steps\",\n",
    "                                        eval_steps=10, logging_steps=50, fp16=False, bf16=False, save_total_limit=2, load_best_model_at_end=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb547c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1935198/1438960544.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=processor,  # processor handles tokenization\n",
    "    model_init=None,             # don't try to reload model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ba66d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/scratch/students/nour/.conda/envs/PDS/lib/python3.11/site-packages/transformers/trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2329\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2330\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2331\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2332\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2333\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/students/nour/.conda/envs/PDS/lib/python3.11/site-packages/transformers/trainer.py:2623\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2621\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2622\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2623\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_samples(epoch_iterator, num_batches, args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2624\u001b[0m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[1;32m   2625\u001b[0m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[1;32m   2626\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_gradient_accumulation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "File \u001b[0;32m/scratch/students/nour/.conda/envs/PDS/lib/python3.11/site-packages/transformers/trainer.py:5603\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[1;32m   5600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_num_items_in_batch:\n\u001b[1;32m   5601\u001b[0m     \u001b[38;5;66;03m# For now we don't support object detection\u001b[39;00m\n\u001b[1;32m   5602\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5603\u001b[0m         num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m))\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batch_samples])\n\u001b[1;32m   5604\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m   5605\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/students/nour/.conda/envs/PDS/lib/python3.11/site-packages/transformers/trainer.py:5603\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   5600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_num_items_in_batch:\n\u001b[1;32m   5601\u001b[0m     \u001b[38;5;66;03m# For now we don't support object detection\u001b[39;00m\n\u001b[1;32m   5602\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5603\u001b[0m         num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m))\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batch_samples])\n\u001b[1;32m   5604\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m   5605\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "with open(\"./gemma3_ner_prototype/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "trainer.save_model(\"./gemma3_ner_prototype\")\n",
    "processor.save_pretrained(\"./gemma3_ner_prototype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1bd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
